remove(KMeansModel)
rxDataStep(inData = paste(strXDF, "Clustering_DS.xdf", sep = ""),
outFile = paste(strXDF, "preClassification1_DS.xdf", sep = ""),
transforms = list(SelectionRatio = as.integer(runif(.rxNumRows,1,11)),
LabelFactorial = factor(Label, c(0,1))),
overwrite=TRUE
)
file.remove(paste(strXDF, "Clustering_DS.xdf", sep = ""))
remove(Clustering_DS)
rxDataStep(inData = paste(strXDF, "preClassification1_DS.xdf", sep = ""),
outFile = paste(strXDF, "preClassification2_DS.xdf", sep = ""),
varsToDrop = c("GeoLocX", "GeoLocY"),
overwrite = TRUE
)
file.remove(paste(strXDF, "preClassification1_DS.xdf", sep = ""))
preClassification2_DS <- RxXdfData(file = paste(strXDF, "preClassification2_DS.xdf", sep = ""))
ClassificationColInfo <- list("LabelFactorial" = list(type = "factor", levels = c("0", "1"), newLevels = c("Cancelled", "Approved")))
Classification_DS <- rxImport(inData = preClassification2_DS,
outFile = paste(strXDF, "Classification_DS.xdf", sep = ""),
colInfo = ClassificationColInfo,
overwrite = TRUE
)
n_Classification <- rxGetInfo(data = Classification_DS)$numRows
remove(ClassificationColInfo)
file.remove(paste(strXDF, "preClassification2_DS.xdf", sep = ""))
remove(preClassification2_DS)
rxHistogram(~LabelFactorial, data = Classification_DS)
rxDataStep(inData = paste(strXDF, "Classification_DS.xdf", sep = ""),
outFile = paste(strXDF, "Training_DS.xdf", sep = ""),
rowSelection = SelectionRatio <= 8, #About 80% of the data
varsToDrop = "SelectionRatio",
blocksPerRead = 20,
rowsPerRead = RowsPerRead,
overwrite = TRUE
)
Training_DS <- RxXdfData(file = paste(strXDF, "Training_DS.xdf", sep = ""))
n_Train <- rxGetInfo(data = Training_DS)$numRows
ActualTrainingPercentage <- n_Train / n_Classification * 100
ActualTrainingPercentage
rxDataStep(inData = paste(strXDF, "Classification_DS.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
rowSelection = SelectionRatio > 8, #About 20% of the data
varsToDrop = "SelectionRatio",
blocksPerRead = 20,
rowsPerRead = RowsPerRead,
overwrite = TRUE
)
Test_DS <- RxXdfData(file = paste(strXDF, "Test_DS.xdf", sep = ""))
n_Test <- rxGetInfo(data = Test_DS)$numRows
ActualTestPercentage <- n_Test / n_Classification * 100
ActualTestPercentage
remove(ActualTestPercentage)
remove(ActualTrainingPercentage)
system.time(
#Tweakable:
#--Dependent Variables
LogisticRegressionModel <- rxLogit(Label ~ TimeSeriesDate + GrafioEktelesisErgou + Katigoria + Xaraktirismos_Ergou + .rxCluster
+ Skopos_Ergou + MelClientDelay + MelDEHDelay + MelOthersDelay + Sinergio_Meletis
+ Ektasi_Ergou + Anagi_YS #+ SAP_Typos_Pelati #+ SAP_Eidos_Aitimatos
+ Mel_Kathisterisi_Pelati + Mel_Kathisterisi_DEH + Mel_Kathisterisi_Triton + Meres_Meletis
+ Kostos_Ergatikon_Kataskevis + Kostos_Ilikon_Kataskevis + Kostos_Kataskevis + Kostos_Ergolavikon_Epidosis
+ Kathisterisi_AitisisKataxorisis + Kathisterisi_Meletis + Kathisterisi_Anagelias + DayOfYearSine
+ DayOfYearCosine + DayOfYearCartesX + DayOfYearCartesY
, data = paste(strXDF, "Training_DS.xdf", sep = "")
, covCoef = TRUE
# ,maxIterations =
# ,fweights = #If duplicate rows have been eliminated, creating a new variable of how many duplicate rows were, then this Variable/Column can be used as Frequency Weight
# ,pweights = #Probablity weights for the observations
,reportProgress = rxGetOption("reportProgress")
,blocksPerRead = rxGetOption("blocksPerRead")
# ,rowSelection =
# ,variableSelection = #rxStepControl(method="stepwise", scope = ~ Age + Start + Number )); parameters that control aspects of stepwise regression; cube must be FALSE
)
)
summary(LogisticRegressionModel)
rxPredict(modelObject = LogisticRegressionModel,
data = paste(strXDF, "Test_DS.xdf", sep = ""),
outData = paste(strXDF, "Test_DS.xdf", sep = ""),
overwrite = TRUE
,predVarNames = "LogRe_PredictionReal"
# ,computeStdErr = TRUE
# ,stdErrorsVarNames = "LogRe_StdError"
# ,interval = "confidence" #to control whether confidence or prediction (tolerance) intervals are computed at the specified level (confLevel). These are sometimes referred to as narrow and wide intervals, respectively
# ,intervalVarNames = c("LogRe_LowerConfInterv", "LogRe_UpperConfInterv")
# ,computeResiduals = TRUE
# ,residVarNames = "LogRe_Residual"
,blocksPerRead = rxGetOption("blocksPerRead")
,reportProgress = rxGetOption("reportProgress")
,xdfCompressionLevel = rxGetOption("xdfCompressionLevel")
)
rxDataStep(inData = paste(strXDF, "Test_DS.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
transforms = list(LogRe_Prediction = as.logical(round(LogRe_PredictionReal))),
overwrite = TRUE
)
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
tmp <- rxCube(~ F(Label):F(LogRe_Prediction), data = paste(strXDF, "Test_DS.xdf", sep = ""))
ShowStatistics(tmp, 3, paste(strXDF, "Training_DS.xdf", sep = ""))
remove(tmp)
rxRocCurve(actualVarName = "Label",
predVarName = "LogRe_PredictionReal",
data = paste(strXDF, "Test_DS.xdf", sep = "")
)
remove(LogisticRegressionModel)
system.time(
#Tweakable:
#--Dependent Variables
#--maxDepth
#--method [anova/class]
#//To be left as is on this experiment (though tweakable if needed)\\#
#-- xval: Cross Validation Folds for Prunning, 10 is great (&very time consuming)
#--cp: 0 is the best (&most time consuming)
TreeModel <- rxDTree(LabelFactorial ~ TimeSeriesDate + GrafioEktelesisErgou + Katigoria + Xaraktirismos_Ergou + .rxCluster
+ Skopos_Ergou + MelClientDelay + MelDEHDelay + MelOthersDelay + Sinergio_Meletis
+ Ektasi_Ergou + Anagi_YS #+ SAP_Typos_Pelati #+ SAP_Eidos_Aitimatos
+ Mel_Kathisterisi_Pelati + Mel_Kathisterisi_DEH + Mel_Kathisterisi_Triton + Meres_Meletis
+ Kostos_Ergatikon_Kataskevis + Kostos_Ilikon_Kataskevis + Kostos_Kataskevis + Kostos_Ergolavikon_Epidosis
+ Kathisterisi_AitisisKataxorisis + Kathisterisi_Meletis + Kathisterisi_Anagelias + DayOfYearSine
+ DayOfYearCosine + DayOfYearCartesX + DayOfYearCartesY
,data = paste(strXDF, "Training_DS.xdf", sep = "")
,xVal = 10 #this controls the number of folds used to perform cross-validation. The default of 2 allows for some pruning; once you have closed in a model you may want to increase the value for final fitting and pruning.
,maxDepth = 15 #this sets the maximum depth of any node of the tree. Computations grow rapidly more expensive as the depth increases, so we recommend a maxDepth of 10 to 15.
,method = "anova"
,maxNumBins = round(min(1001, max(101, sqrt(n_Train)))) #this controls the maximum number of bins used for each variable. Managing the number of bins is important in controlling memory usage. The default is min(1001, max(101, sqrt(num of obs))). For small data sets with continuous predictors, you may find that you need to increase the maxNumBins to obtain models that resemble those from rpart.
#For large data sets (100000 or more observations), you may need to adjust the following parameters to obtain meaningful models:
#The default cp of 0 produces a very large number of splits; specifying cp = 1e-5 produces a more manageable set of splits in this model
,cp = 0 #this is a complexity parameter and sets the bar for how much a split must reduce the complexity before being accepted. We have set the default to 0 and recommend using maxDepth and minBucket to control your tree sizes. If you want to specify a cp value, start with a conservative value, such as rpart’s 0.01; if you don’t see an adequate number of splits, decrease the cp by powers of 10 until you do. For our large airline data, we have found interesting models begin with a cp of about 1e-4.
,pruneCp = "auto"
#,maxCompete = 0 #this specifies the number of “competitor splits” retained in the output. By default, rxDTree sets this to 0, but a setting of 3 or 4 can be useful for diagnostic purposes in determining why a particular split was chosen.
#,useSurrogate = #0, 1 or 2
#,maxSurrogate = 0 #this specifies the number of surrogate splits retained in the output. Again, by default rxDTree sets this to 0. Surrogate splits are used to assign an observation when the primary split variable is missing for that observation.
#,surrogateStyle = #0 or 1, 0 penalises surrogates with many missing values
#,minSplit = #determines how many observations must be in a node before a split is attempted
#,minBucket =  #determines how many observations must remain in a terminal node.
#,fweights = #If duplicate rows have been eliminated, creating a new variable of how many duplicate rows were, then this Variable/Column can be used as Frequency Weight
#,pweights = #Probablity weights for the observations
#,cost = c("") #a vector of non-negative costs, containing one element for each variable in the model. Defaults to one for all variables. When deciding which split	to choose, the improvement on splitting on a variable is divided by its cost
#,parms = list(loss = c(0, 3, 1, 0))
,blocksPerRead = rxGetOption("blocksPerRead")
,reportProgress = rxGetOption("reportProgress")
,xdfCompressionLevel = rxGetOption("xdfCompressionLevel")
# ,rowSelection = #name of a logical variable in the data set (in quotes) or a logical expression using variables in the data set to specify row selection.
)
)
TreeModel #The Tree model
TreeModel$variable.importance   #[vector] A numerical value representing how important the variable has been to the model
printcp(rxAddInheritance(TreeModel)) #Table of optimal prunings based on complexity
plotcp(rxAddInheritance(TreeModel))
rxPredict(modelObject = TreeModel,
data = paste(strXDF, "Test_DS.xdf", sep = ""),
outData = paste(strXDF, "Test_DS.xdf", sep = ""),
overwrite = TRUE,
predVarNames = "Tree_PredictionReal"
# ,computeStdErr = TRUE
# ,stdErrorsVarNames = "LogRe_StdError"
# ,interval = "confidence" #to control whether confidence or prediction (tolerance) intervals are computed at the specified level (confLevel). These are sometimes referred to as narrow and wide intervals, respectively
# ,intervalVarNames = c("LogRe_LowerConfInterv", "LogRe_UpperConfInterv")
# ,computeResiduals = TRUE
# ,residVarNames = "LogRe_Residual"
,blocksPerRead = rxGetOption("blocksPerRead")
,reportProgress = rxGetOption("reportProgress")
,xdfCompressionLevel = rxGetOption("xdfCompressionLevel")
)
rxDataStep(inData = paste(strXDF, "Test_DS.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
transforms = list(Tree_Prediction = as.logical(round(Tree_PredictionReal))),
overwrite = TRUE
)
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
tmp <- rxCube(~ F(Label):F(Tree_Prediction), data = paste(strXDF, "Test_DS.xdf", sep = ""))
ShowStatistics(tmp, 3, paste(strXDF, "Test_DS.xdf", sep = ""))
remove(tmp)
rxRocCurve(actualVarName = "Label",
predVarName = "Tree_PredictionReal",
data = paste(strXDF, "Test_DS.xdf", sep = ""),
title = "Decision Tree ROC Curve"
)
plot(createTreeView(TreeModel))
remove(TreeModel)
system.time(
#Tweakable:
#--Dependent Variables
#--smoothingFactor
#//To be left as is on this experiment (though tweakable if needed)\\#
NaiveBayesModel <- rxNaiveBayes(LabelFactorial ~ TimeSeriesDate + GrafioEktelesisErgou + Katigoria + Xaraktirismos_Ergou + .rxCluster
+ Skopos_Ergou + MelClientDelay + MelDEHDelay + MelOthersDelay + Sinergio_Meletis
+ Ektasi_Ergou + Anagi_YS #+ SAP_Typos_Pelati #+ SAP_Eidos_Aitimatos
+ Mel_Kathisterisi_Pelati + Mel_Kathisterisi_DEH + Mel_Kathisterisi_Triton + Meres_Meletis
+ Kostos_Ergatikon_Kataskevis + Kostos_Ilikon_Kataskevis + Kostos_Kataskevis + Kostos_Ergolavikon_Epidosis
+ Kathisterisi_AitisisKataxorisis + Kathisterisi_Meletis + Kathisterisi_Anagelias + DayOfYearSine
+ DayOfYearCosine + DayOfYearCartesX + DayOfYearCartesY
,data = paste(strXDF, "Training_DS.xdf", sep = "")
,smoothingFactor = 1 #If we try to use our classifier on the test data without specifying a smoothing factor in our call to rxNaiveBayes the function rxPredict produces no results since our test data only has data from 2009. In general, smoothing is used to avoid overfitting your model. It follows that to achieve the optimal classifier you may want to smooth the conditional probabilities even if every level of each variable is observed. perform Laplace smoothing. A positive smoothing factor to account for cases not present in the training data. It avoids modeling issues by preventing zero conditional probability estimates. Since the conditional probabilities are being multiplied in the model, adding a small number to 0 probabilities, precludes missing categories from wiping out the calculation.
#,fweights = #If duplicate rows have been eliminated, creating a new variable of how many duplicate rows were, then this Variable/Column can be used as Frequency Weight
#,pweights = #Probablity weights for the observations
,blocksPerRead = rxGetOption("blocksPerRead")
,reportProgress = rxGetOption("reportProgress")
,xdfCompressionLevel = rxGetOption("xdfCompressionLevel")
# ,rowSelection = #name of a logical variable in the data set (in quotes) or a logical expression using variables in the data set to specify row selection.
)
)
NaiveBayesModel #The Naive Bayes model
NB_Pred <- rxPredict(modelObject = NaiveBayesModel,
data = paste(strXDF, "Test_DS.xdf", sep = ""),
outData = paste(strXDF, "Test_DS.xdf", sep = ""),
overwrite = TRUE
,predVarNames = c("NBCancelledProbabil", "NB_PredictionReal")#, "tmpNB_Prediction")
,type = "prob" #To get probabilities instead of 0/1
# ,computeStdErr = TRUE
# ,stdErrorsVarNames = "LogRe_StdError"
# ,interval = "confidence" #to control whether confidence or prediction (tolerance) intervals are computed at the specified level (confLevel). These are sometimes referred to as narrow and wide intervals, respectively
# ,intervalVarNames = c("LogRe_LowerConfInterv", "LogRe_UpperConfInterv")
# ,computeResiduals = TRUE
# ,residVarNames = "LogRe_Residual"
,blocksPerRead = rxGetOption("blocksPerRead")
,reportProgress = rxGetOption("reportProgress")
,xdfCompressionLevel = rxGetOption("xdfCompressionLevel")
)
rxDataStep(inData = paste(strXDF, "Test_DS.xdf", sep = ""),
outFile = paste(strXDF, "tmp.xdf", sep = ""),
transforms = list(NB_Prediction = as.logical(round(NB_PredictionReal))),
overwrite = TRUE
)
rxDataStep(inData = paste(strXDF, "tmp.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
varsToDrop = c("NBCancelledProbabil"),
overwrite = TRUE
)
file.remove(paste(strXDF, "tmp.xdf", sep = ""))
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame           #Naive Bayes has 35 missing values on its output
tmp <- rxCube(~ F(LabelFactorial):F(NB_Prediction), data = paste(strXDF, "Test_DS.xdf", sep = ""))
ShowStatistics(tmp, 3, paste(strXDF, "Test_DS.xdf", sep = ""))
remove(tmp)
rxRocCurve(actualVarName = "Label",
predVarName = "NB_PredictionReal",
data = paste(strXDF, "Test_DS.xdf", sep = ""),
title = "Naive Bayes ROC Curve"
)
remove(NaiveBayesModel)
system.time(
#Tweakable:
#-- Variables
#--lossFunction [logLoss,hingeLoss,smoothHingeLoss|BinaryClass/squaredLoss|LinearRegr]
#--type [binary/regression]
#--l2Weight
#--l1Weight
#--
#--
#//To be left as is on this experiment (though tweakable if needed)\\#
SDCAModel <- rxFastLinear(Label ~ TimeSeriesDate + GrafioEktelesisErgou + Katigoria + Xaraktirismos_Ergou + .rxCluster
+ Skopos_Ergou + MelClientDelay + MelDEHDelay + MelOthersDelay + Sinergio_Meletis
+ Ektasi_Ergou + Anagi_YS #+ SAP_Typos_Pelati #+ SAP_Eidos_Aitimatos
+ Mel_Kathisterisi_Pelati + Mel_Kathisterisi_DEH + Mel_Kathisterisi_Triton + Meres_Meletis
+ Kostos_Ergatikon_Kataskevis + Kostos_Ilikon_Kataskevis + Kostos_Kataskevis + Kostos_Ergolavikon_Epidosis
+ Kathisterisi_AitisisKataxorisis + Kathisterisi_Meletis + Kathisterisi_Anagelias + DayOfYearSine
+ DayOfYearCosine + DayOfYearCartesX + DayOfYearCartesY
, data = paste(strXDF, "Training_DS.xdf", sep = "")
#For large data sets (100000 or more observations), you may need to adjust the following parameters to obtain meaningful models:
#The default cp of 0 produces a very large number of splits; specifying cp = 1e-5 produces a more manageable set of splits in this model
,type = "binary"
,convergenceTolerance = 0.1 #Specifies the tolerance threshold used as a convergence criterion. It must be between 0 and 1. The default value is 0.1. The algorithm is considered to have converged if the relative duality gap, which is the ratio between the duality gap and the primal loss, falls below the specified convergence tolerance.
,normalize = "auto" #Specifies the type of automatic normalization used: [auto/no/yes/warn] "warn": if normalization is needed, a warning message is displayed, but normalization is not performed. Normalization rescales disparate data ranges to a standard scale. Feature scaling insures the distances between data points are proportional and enables various optimization methods such as gradient descent to converge much faster. If normalization is performed, a MaxMin normalizer is used. It normalizes values in an interval [a, b] where -1 <= a <= 0 and 0 <= b <= 1 and b - a = 1. This normalizer preserves sparsity by mapping zero to zero
#,maxIterations = 25 #Specifies an upper bound on the number of training iterations. This parameter must be positive or NULL. If NULL is specified, the actual value is automatically computed based on data set. Each iteration requires a complete pass over the training data. Training terminates after the total number of iterations reaches the specified upper bound or when the loss function converges, whichever happens earlier.
#,lossFunction = #Specifies the empirical loss function to optimize. For binary classification, the following choices are available: logLoss: The log-loss. This is the default. hingeLoss: The SVM hinge loss. Its parameter represents the margin size. smoothHingeLoss: The smoothed hinge loss. Its parameter represents the smoothing constant. For linear regression, squared loss squaredLoss is currently supported.
#,l2Weight = #Specifies the L2 regularization weight. The value must be either non-negative or NULL. If NULL is specified, the actual value is automatically computed based on data set
#,l1Weight = #Specifies the L1 regularization weight. The value must be either non-negative or NULL. If NULL is specified, the actual value is automatically computed based on data set
#,shuffle = FALSE #Specifies whether to shuffle the training data. Set TRUE to shuffle the data; FALSE not to shuffle. The default value is FALSE. SDCA is a stochastic optimization algorithm. If shuffling is turned on, the training data is shuffled on each iteration.
#
,blocksPerRead = rxGetOption("blocksPerRead")
,reportProgress = rxGetOption("reportProgress")
# ,rowSelection = #name of a logical variable in the data set (in quotes) or a logical expression using variables in the data set to specify row selection.
)
)
summary(SDCAModel) #The Stochastic Dual Coordinate Ascend Model
SDCAModel$coefficients
rxPredict(modelObject = SDCAModel,
data = paste(strXDF, "Test_DS.xdf", sep = ""),
outData = paste(strXDF, "tmp.xdf", sep = ""),
overwrite = TRUE
)
tmpVarInfo <- list(
PredictedLabel = list(newName = SDCA_Prediction),
Probability.1 = list(newName = SDCA_PredictionReal)
)
tmpVarInfo <- list(
PredictedLabel = list(newName = "SDCA_Prediction"),
Probability.1 = list(newName = "SDCA_PredictionReal")
)
rxSetVarInfo(varInfo = tmpVarInfo,
data = paste(strXDF, "tmp.xdf", sep = "")
)
rxGetVarInfo( paste(strXDF, "tmp.xdf", sep = "") )
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
rxPredict(modelObject = SDCAModel,
data = paste(strXDF, "Test_DS.xdf", sep = ""),
outData = paste(strXDF, "tmp.xdf", sep = ""),
extraVarsToWrite = "ID_Erga",
overwrite = TRUE
)
tmpVarInfo <- list(
PredictedLabel = list(newName = "SDCA_Prediction"),
Probability.1 = list(newName = "SDCA_PredictionReal")
)
rxSetVarInfo(varInfo = tmpVarInfo,
data = paste(strXDF, "tmp.xdf", sep = "")
)
rxDataStep(inData = paste(strXDF, "tmp.xdf", sep = ""),
outFile = paste(strXDF, "tmp2.xdf", sep = ""),
varsToKeep = c("SDCA_PredictionReal", "SDCA_Prediction")
)
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
rxMerge(inData1 = paste(strXDF, "Test_DS.xdf", sep = ""),
inData2 = paste(strXDF, "tmp2.xdf", sep = ""),
type = "left",
matchVars = c("ID_Erga")
)
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
rxSummary(~., data = paste(strXDF, "tmp2.xdf", sep = ""))$sDataFrame
rxMerge(inData1 = paste(strXDF, "Test_DS.xdf", sep = ""),
inData2 = paste(strXDF, "tmp2.xdf", sep = ""),
type = "oneToOne"
)
rxSummary(~., data = paste(strXDF, "tmp2.xdf", sep = ""))$sDataFrame
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
rxGetInfo(paste(strXDF, "Test_DS.xdf", sep = ""), getVarInfo = TRUE, numRows = 0)
rocOut <- rxRoc(actualVarName = "Label",
predVarName = c("LogRe_PredictionReal",
"Tree_PredictionReal",
"NB_PredictionReal",
"RF_PredictionReal",
"StochGB_PredictionReal"),
data = paste(strXDF, "Test_DS.xdf", sep = "")
)
rocOut <- rxRoc(actualVarName = "Label",
predVarName = c("LogRe_PredictionReal",
"Tree_PredictionReal",
"NB_PredictionReal",
#"RF_PredictionReal",
#"StochGB_PredictionReal"),
data = paste(strXDF, "Test_DS.xdf", sep = "")
)
rocOut <- rxRoc(actualVarName = "Label",
predVarName = c("LogRe_PredictionReal",
"Tree_PredictionReal",
"NB_PredictionReal"
#,"RF_PredictionReal",
#"StochGB_PredictionReal"
),
data = paste(strXDF, "Test_DS.xdf", sep = "")
)
rocOut <- rxRoc(actualVarName = "Label",
predVarName = c("LogRe_PredictionReal",
"Tree_PredictionReal",
"NB_PredictionReal"
#,"RF_PredictionReal",
#"StochGB_PredictionReal"
),
data = paste(strXDF, "Test_DS.xdf", sep = "")
)
rocOut
rxAuc(rocOut)
plot(rocOut,
title = "ROC Curve for Label",
lineStyle = c("solid", "twodash", "dashed")
)
rxSummary(~., data = paste(strXDF, "tmp2.xdf", sep = ""))$sDataFrame
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
rxSummary(~., data = paste(strXDF, "tmp2.xdf", sep = ""))$sDataFrame
rxSummary(~., data = rxMerge(inData1 = paste(strXDF, "Test_DS.xdf", sep = ""),
inData2 = paste(strXDF, "tmp2.xdf", sep = ""),
type = "oneToOne"
))$sDataFrame
?rxMerge
rxMerge(inData1 = paste(strXDF, "Test_DS.xdf", sep = ""),
inData2 = paste(strXDF, "tmp2.xdf", sep = ""),
outFile = paste(strXDF, "tmp.xdf", sep = ""),
type = "oneToOne"
)
rxMerge(inData1 = paste(strXDF, "Test_DS.xdf", sep = ""),
inData2 = paste(strXDF, "tmp2.xdf", sep = ""),
outFile = paste(strXDF, "tmp.xdf", sep = ""),
type = "oneToOne",
overwrite = TRUE
)
Test_DS <- rxImport(inData = paste(strXDF, "tmp.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
rowsPerRead = RowsPerRead,
overwrite = TRUE
)
remove(tmpVarInfo)
file.remove(paste(strXDF, "tmp.xdf", sep = ""))
file.remove(paste(strXDF, "tmp2.xdf", sep = ""))
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
tmp <- rxCube(~ F(Label):F(SDCA_Prediction), data = paste(strXDF, "Test_DS.xdf", sep = ""))
ShowStatistics(tmp, 3, Test_DS)
remove(tmp)
plot(SDCAModel)           #How the Out-Of-Bags error decreases with number of trees
rxVarImpPlot(SDCAModel)   #Importance of Variables for the model
rxRocCurve(actualVarName = "Label",
predVarName = "SDCA_PredictionReal",
data = paste(strXDF, "Test_DS.xdf", sep = ""),
chanceGridLine = TRUE
)
remove(SDCAModel)
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
rxDataStep(inData = paste(strXDF, "Classification_DS.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
rowSelection = SelectionRatio > 8, #About 20% of the data
varsToDrop = "SelectionRatio",
blocksPerRead = 20,
rowsPerRead = RowsPerRead,
overwrite = TRUE
)
Test_DS <- RxXdfData(file = paste(strXDF, "Test_DS.xdf", sep = ""))
n_Test <- rxGetInfo(data = Test_DS)$numRows
ActualTestPercentage <- n_Test / n_Classification * 100
ActualTestPercentage
remove(ActualTestPercentage)
?rxNeuralNet
library(rpart) #No installation needed, is already installed
library(rpart.plot)
library(rattle)
library(RevoTreeView) #No installation needed, is already installed
library(MicrosoftML)  #No
?rxNeuralNet
NNModel <- rxNeuralNet(Label ~ TimeSeriesDate + GrafioEktelesisErgou + Katigoria + Xaraktirismos_Ergou + .rxCluster
+ Skopos_Ergou + MelClientDelay + MelDEHDelay + MelOthersDelay + Sinergio_Meletis
+ Ektasi_Ergou + Anagi_YS #+ SAP_Typos_Pelati #+ SAP_Eidos_Aitimatos
+ Mel_Kathisterisi_Pelati + Mel_Kathisterisi_DEH + Mel_Kathisterisi_Triton + Meres_Meletis
+ Kostos_Ergatikon_Kataskevis + Kostos_Ilikon_Kataskevis + Kostos_Kataskevis + Kostos_Ergolavikon_Epidosis
+ Kathisterisi_AitisisKataxorisis + Kathisterisi_Meletis + Kathisterisi_Anagelias + DayOfYearSine
+ DayOfYearCosine + DayOfYearCartesX + DayOfYearCartesY
, data = paste(strXDF, "Training_DS.xdf", sep = "")
,type = "binary"
,numHiddenNodes = 1000 #The default number of hidden nodes in the neural net. The default value is 100
,numIterations = 100 #The number of iterations on the full training set. The default value is 100
,acceleration = "gpu" #[sse/gpu]
,optimizer = sgd() #[sgd()/adaDeltaSgd()] A list specifying either the sgd or adaptive optimization algorithm. This list can be created using sgd or adaDeltaSgd. The default value is sgd.
# ,miniBatchSize = 1 #[1/256] Sets the mini-batch size. Recommended values are between 1 and 256. This parameter is only used when the acceleration is GPU. Setting this parameter to a higher value improves the speed of training, but it might negatively affect the accuracy. The default value is 1.
# ,netDefinition = #The Net# definition of the structure of the neural network. For more information about the Net# language, see https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-azure-ml-netsharp-reference-guide
# ,initWtsDiameter = 0.1 #Sets the initial weights diameter that specifies the range from which values are drawn for the initial learning weights. The weights are initialized randomly from within this range. The default value is 0.1.
# ,maxNorm = #Specifies an upper bound to constrain the norm of the incoming weight vector at each hidden unit. This can be very important in maxout neural networks as well as in cases where training produces unbounded weights
# ,normalize = "auto" #[auto/no/yes/warn] "warn": if normalization is needed, a warning message is displayed, but normalization is not performed. Normalization rescales disparate data ranges to a standard scale. Feature scaling insures the distances between data points are proportional and enables various optimization methods such as gradient descent to converge much faster. If normalization is performed, a MaxMin normalizer is used. It normalizes values in an interval [a, b] where -1 <= a <= 0 and 0 <= b <= 1 and b - a = 1. This normalizer preserves sparsity by mapping zero to zero
,blocksPerRead = rxGetOption("blocksPerRead")
,reportProgress = rxGetOption("reportProgress")
# ,rowSelection = #name of a logical variable in the data set (in quotes) or a logical expression using variables in the data set to specify row selection.
)
summary(NNModel) #The Stochastic Dual Coordinate Ascend Model
rxPredict(modelObject = NNModel,
data = paste(strXDF, "Test_DS.xdf", sep = ""),
outData = paste(strXDF, "tmp.xdf", sep = ""),
overwrite = TRUE
)
tmpVarInfo <- list(
PredictedLabel = list(newName = "NN_Prediction"),
Probability.1 = list(newName = "NN_PredictionReal")
)
rxSetVarInfo(varInfo = tmpVarInfo,
data = paste(strXDF, "tmp.xdf", sep = "")
)
rxDataStep(inData = paste(strXDF, "tmp.xdf", sep = ""),
outFile = paste(strXDF, "tmp2.xdf", sep = ""),
varsToKeep = c("NN_PredictionReal", "NN_Prediction"),
overwrite = TRUE
)
CurVarNamesTest <- rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame[[1]]
if ("NN_PredictionReal" %in% (CurVarNamesTest)) {
rxDataStep(inData = paste(strXDF, "Test_DS.xdf", sep = ""),
outFile = paste(strXDF, "tmp3.xdf", sep = ""),
varsToDrop = c("NN_PredictionReal", "NN_Prediction"),
overwrite = TRUE
)
Test_DS <- rxImport(inData = paste(strXDF, "tmp3.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
rowsPerRead = RowsPerRead,
overwrite = TRUE
)
file.remove(paste(strXDF, "tmp3.xdf", sep = ""))
}
remove(CurVarNamesTest)
rxMerge(inData1 = paste(strXDF, "Test_DS.xdf", sep = ""),
inData2 = paste(strXDF, "tmp2.xdf", sep = ""),
outFile = paste(strXDF, "tmp.xdf", sep = ""),
type = "oneToOne",
overwrite = TRUE
)
Test_DS <- rxImport(inData = paste(strXDF, "tmp.xdf", sep = ""),
outFile = paste(strXDF, "Test_DS.xdf", sep = ""),
rowsPerRead = RowsPerRead,
overwrite = TRUE
)
remove(tmpVarInfo)
file.remove(paste(strXDF, "tmp.xdf", sep = ""))
file.remove(paste(strXDF, "tmp2.xdf", sep = ""))
rxSummary(~., data = paste(strXDF, "Test_DS.xdf", sep = ""))$sDataFrame
tmp <- rxCube(~ F(Label):F(NN_Prediction), data = paste(strXDF, "Test_DS.xdf", sep = ""))
ShowStatistics(tmp, 3, Test_DS)
remove(tmp)
rxRocCurve(actualVarName = "Label",
predVarName = "NN_PredictionReal",
data = paste(strXDF, "Test_DS.xdf", sep = ""),
chanceGridLine = TRUE,
title = "Neural Networks"
)
remove(NNModel)
remove(rocOut)
remove(data)
remove(n_Classification)
remove(k)
remove(n_Test)
remove(n_Train)
remove(NB_Pred)
